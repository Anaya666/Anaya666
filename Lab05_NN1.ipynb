{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anaya666/Anaya666/blob/main/Lab05_NN1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1368e02-69f0-463e-9107-c7c37e6e0ac0",
      "metadata": {
        "id": "c1368e02-69f0-463e-9107-c7c37e6e0ac0"
      },
      "source": [
        "# ðŸ§ª LAB: Manual MLPs for Classification and Regression\n",
        "\n",
        "In this lab, you will use `PyTorch` to implement manually a multi-layer perceptron (MLP) for three different tasks: binary classification, multi-class classification and regression.\n",
        "\n",
        "## General instructions to complete in ALL three tasks:\n",
        "\n",
        "1. ***IMPLEMENTATION***:\n",
        "   \n",
        "Implement a separate class for each task:\n",
        "\n",
        "  - `BinaryMLP` for the binary classification task  \n",
        "  - `MultiClassMLP` for the multi-class classification task  \n",
        "  - `RegressionMLP` for the regression task\n",
        "\n",
        "   Each class must include the following methods:\n",
        "\n",
        "  - `__init__` for initializing 1 or 2 hidden layers.\n",
        "  - `forward` to transfer information from the input to the output layer.\n",
        "  - `cost` computing the cost.\n",
        "  - `fit` for training, using autograd and manual updates. **Use stochastic gradient descent to update your weights**. *N.B.* You may probably reuse much of the code we used in this week tutorial already.\n",
        "  - `predict` to convert the information at the output layer into the required output.\n",
        "\n",
        "2. ***DATA PREPARATION***\n",
        "\n",
        "For each task, you will be provided with a toy dataset. For each dataset:\n",
        "\n",
        "  - Split into training and test sets (use an 80/20 split)\n",
        "  - Standardize the features properly, avoiding data leakage\n",
        "  - Convert all data into `PyTorch` Tensors for compatibility\n",
        "\n",
        "3. ***MODEL TRAINING AND EVALUATION***\n",
        "\n",
        "Instantiate the model for each task and train it under different hyperparameter configurations. In each case, record the performance on both the training and test sets using the appropriate metric for the task (accuracy, MSE, etc.). You should explore the following configurations:\n",
        "\n",
        "  - One hidden layer, varying the number of hidden units (use ReLU as their activation function)\n",
        "  - Same number of hidden units across one or two hidden layers (use ReLU as their activation function)\n",
        "  - Repeat the above setups using Tanh activation instead of ReLU\n",
        "\n",
        "Present your results in a compact way (e.g. a summary table, a data frame etc).\n",
        "\n",
        "**NOTE**: When training your model, use a fixed learning rate of your choice (e.g., 0.01 is a reasonable starting point) and a reasonably large number of  epochs (e.g., 100â€“200) based on how training and test performance evolve.\n",
        "\n",
        "4. ***REFLECTION AND DISCUSSION***\n",
        "\n",
        "Reflect on the impact of the different hyperparameter settings:\n",
        "\n",
        "- How does the number of hidden units affect performance?\n",
        "- What changes when using two layers instead of one?\n",
        "- How does the activation function (ReLU vs. Tanh) influence results?\n",
        "\n",
        "Please elaborate your answers.\n",
        "\n",
        "---\n",
        "\n",
        "**Collaboration Note**: This assignment is designed to support collaborative work. We encourage you to divide tasks among group members so that everyone can contribute meaningfully. Many components of the assignment can be approached in parallel or split logically across team members. Good coordination and thoughtful integration of your work will lead to a stronger final result.\n",
        "\n",
        "**Ideally, each group member should be responsible for one of the separate tasks.** BUT, everyone should help each other along the way, both reviewing and refining results and discussion.\n",
        "\n",
        "---\n",
        "\n",
        "In total, this lab assignment will be worth **100 points**.\n",
        "\n",
        "---\n",
        "**Submission notes**:\n",
        "\n",
        "* Write down all group members' names, or at least the group name (if you have one and you previously provided it), in the first cell of the notebook.\n",
        "\n",
        "* Verify that the notebook runs as expected and that all required outputs are included.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcb1b88b-ed51-4873-af88-211ff5c2ae00",
      "metadata": {
        "id": "dcb1b88b-ed51-4873-af88-211ff5c2ae00"
      },
      "outputs": [],
      "source": [
        "NAME(s) ="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8971e94-ed43-45b6-9031-28917379a09d",
      "metadata": {
        "id": "e8971e94-ed43-45b6-9031-28917379a09d"
      },
      "source": [
        "## 1. Pre-implementation Group Discussion (15 points)\n",
        "\n",
        "Discuss and agree on:\n",
        "\n",
        "- What cost function should be used for each of the below task.\n",
        "- What changes are needed in the output layer for each of these tasks. In particular, consider the number of units and the activation function.\n",
        "- Why it is important to standardize the data before training each model.\n",
        "- How you could detect overfitting when training your models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09da4030-2a13-4ce8-ab11-2ae7cbadd360",
      "metadata": {
        "id": "09da4030-2a13-4ce8-ab11-2ae7cbadd360"
      },
      "source": [
        "1.\n",
        "- Binary function: Binary Cross Entropy Cost\n",
        "- multi-class classification: categorical cross entropy loss\n",
        "- Regression: Mean Squared error\n",
        "2.\n",
        "- Binary classification: 1 output unit (either 0 or 1), Sigmoid activation function\n",
        "- Multi-class classification: 3 output units, activation function: Softmax\n",
        "- Regression: 1 output unit (since we are trying to predict 1 continuous value), No activation function - no identity.\n",
        "3. To keep it on the same scale and prevent numerically larger data points from exerting more weight in the model.\n",
        "4.\n",
        "- Binary: high accuracy on training data, low in test data\n",
        "- Multi-class: accuracy, low in test data\n",
        "- Regression: if R^2 is very high on training data but low on test data, indicating poor generalisability."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5930028c-aa66-43ef-8ab2-357449c50cf1",
      "metadata": {
        "id": "5930028c-aa66-43ef-8ab2-357449c50cf1"
      },
      "source": [
        "## 2. Binary Classification (25 points)\n",
        "\n",
        "Use the dataset below to complete points 1 to 4 in the general instructions for this task.\n",
        "\n",
        "Use as many cells as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "138ef9eb-7e81-4af9-809d-dc672e4240ae",
      "metadata": {
        "id": "138ef9eb-7e81-4af9-809d-dc672e4240ae"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb2961c1-dade-42e1-a07a-9118909a81fa",
      "metadata": {
        "id": "fb2961c1-dade-42e1-a07a-9118909a81fa"
      },
      "outputs": [],
      "source": [
        "# USE AS MANY CELLS AS NEEDED\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2903c018-71db-40be-b40f-2d9c3814d016",
      "metadata": {
        "id": "2903c018-71db-40be-b40f-2d9c3814d016"
      },
      "source": [
        "## 2. Multi-Class Classification (25 points)\n",
        "\n",
        "Use the dataset below to complete points 1 to 4 in the general instructions for this task.\n",
        "\n",
        "Use as many cells as needed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff1e6e4c-48df-45c9-9146-4448f3d604e0",
      "metadata": {
        "id": "ff1e6e4c-48df-45c9-9146-4448f3d604e0"
      },
      "source": [
        "---\n",
        "**NOTE**: This will likely be the most challenging exercise. To help you, here are some pointers:\n",
        "\n",
        " - The **output (last) layer** should have as many units as there are classes in your data.  \n",
        "\n",
        " - The **activation function of the output layer must be softmax**, not sigmoid. Softmax ensures that all output values are between 0 and 1 and sum to 1, so they can be interpreted as probabilities across the different classes.  \n",
        "\n",
        " - For a multi-class problem, your target variable `y` should be **one-hot encoded**. For example:  \n",
        "   - Label = 0 â†’ [1, 0, 0]  \n",
        "   - Label = 1 â†’ [0, 1, 0]  \n",
        "   - Label = 2 â†’ [0, 0, 1]  \n",
        "   You can easily achieve this with `OneHotEncoder` from `sklearn.preprocessing`.  \n",
        "\n",
        " - The **predicted class** corresponds to the unit with the highest probability.  \n",
        "   Example:  \n",
        "   - `[0.1, 0.3, 0.6] â†’ class 2`  \n",
        "   - `[0.6, 0.2, 0.2] â†’ class 0`  \n",
        "\n",
        " - For this exercise, you need to **implement the categorical cross-entropy loss** (an extension of binary cross-entropy to multiple classes). It is defined as:  \n",
        "\n",
        "   $$\\sum_{c=1}^{l} y_{o,c}\\,\\log(p_{o,c}),$$\n",
        "\n",
        "   where $l$ is the number of classes, $y_{o,c}$ is the one-hot encoded label for observation $o$, and $p_{o,c}$ is the predicted probability for class $c$ (after applying softmax). The log is the natural logarithm.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import torch"
      ],
      "metadata": {
        "id": "Ii65ZLrtGQgp"
      },
      "id": "Ii65ZLrtGQgp",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3980b88d-e3b0-4ede-bfa4-d854e2842201",
      "metadata": {
        "id": "3980b88d-e3b0-4ede-bfa4-d854e2842201"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_classes=3,\n",
        "                           n_clusters_per_class=1,\n",
        "                           n_features=2, n_informative=2, n_redundant=0, random_state=1234, flip_y=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3d05c067-aece-4c59-bf0c-0a994ef7690c",
      "metadata": {
        "id": "3d05c067-aece-4c59-bf0c-0a994ef7690c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class MultiClassMLP(object):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim, learning_rate=0.1):\n",
        "        \"\"\"\n",
        "        Initialize a 2-hidden-layer multi-class MLP.\n",
        "        Args:\n",
        "            input_dim: number of input features\n",
        "            hidden_dim1: number of units in first hidden layer\n",
        "            hidden_dim2: number of units in second hidden layer\n",
        "            output_dim: number of output classes\n",
        "            learning_rate: learning rate for SGD\n",
        "        \"\"\"\n",
        "        # First hidden layer\n",
        "        self.W_1 = torch.randn(input_dim, hidden_dim1, requires_grad=True)\n",
        "        self.b_1 = torch.randn(hidden_dim1, requires_grad=True)\n",
        "\n",
        "        # Second hidden layer\n",
        "        self.W_2 = torch.randn(hidden_dim1, hidden_dim2, requires_grad=True)\n",
        "        self.b_2 = torch.randn(hidden_dim2, requires_grad=True)\n",
        "\n",
        "        # Output layer\n",
        "        self.W_out = torch.randn(hidden_dim2, output_dim, requires_grad=True)\n",
        "        self.b_out = torch.randn(output_dim, requires_grad=True)\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    # ---------------- Activation functions ----------------\n",
        "    def relu(self, z):\n",
        "        return torch.maximum(z, torch.tensor(0.0))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        return torch.exp(x) / torch.exp(x).sum(dim=1, keepdim=True)\n",
        "\n",
        "    # ---------------- Forward pass ----------------\n",
        "    def forward(self, X):\n",
        "        # First hidden layer\n",
        "        h1 = torch.matmul(X, self.W_1) + self.b_1\n",
        "        h1_relu = self.relu(h1)\n",
        "\n",
        "        # Second hidden layer\n",
        "        h2 = torch.matmul(h1_relu, self.W_2) + self.b_2\n",
        "        h2_relu = self.relu(h2)\n",
        "\n",
        "        # Output layer\n",
        "        out = torch.matmul(h2_relu, self.W_out) + self.b_out\n",
        "\n",
        "        # Softmax probabilities\n",
        "        return self.softmax(out)\n",
        "\n",
        "    # ---------------- Loss ----------------\n",
        "    def categorical_cross_entropy(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Compute categorical cross-entropy loss.\n",
        "        y_pred: model predictions after softmax, shape (N, C)\n",
        "        y_true: one-hot encoded true labels, shape (N, C)\n",
        "        \"\"\"\n",
        "        eps = torch.finfo(y_pred.dtype).eps\n",
        "        y_pred = torch.clamp(y_pred, eps, 1 - eps)\n",
        "        loss = -torch.mean(torch.sum(y_true * torch.log(y_pred), dim=1))\n",
        "        return loss\n",
        "\n",
        "    # ---------------- Training ----------------\n",
        "    def fit(self, X, y, n_epochs=100):\n",
        "        for epoch in range(n_epochs):\n",
        "            randperm = torch.randperm(X.shape[0])\n",
        "            for ii in randperm:\n",
        "                x_batch = X[ii].unsqueeze(0)\n",
        "                y_batch = y[ii].unsqueeze(0)\n",
        "\n",
        "                # Forward pass\n",
        "                probs = self.forward(x_batch)\n",
        "\n",
        "                # Compute loss\n",
        "                cost = self.categorical_cross_entropy(probs, y_batch)\n",
        "\n",
        "                # Backward pass\n",
        "                cost.backward()\n",
        "\n",
        "                # Gradient descent update\n",
        "                with torch.no_grad():\n",
        "                    self.W_1 -= self.learning_rate * self.W_1.grad\n",
        "                    self.b_1 -= self.learning_rate * self.b_1.grad\n",
        "                    self.W_2 -= self.learning_rate * self.W_2.grad\n",
        "                    self.b_2 -= self.learning_rate * self.b_2.grad\n",
        "                    self.W_out -= self.learning_rate * self.W_out.grad\n",
        "                    self.b_out -= self.learning_rate * self.b_out.grad\n",
        "\n",
        "                    # Zero gradients\n",
        "                    self.W_1.grad.zero_()\n",
        "                    self.b_1.grad.zero_()\n",
        "                    self.W_2.grad.zero_()\n",
        "                    self.b_2.grad.zero_()\n",
        "                    self.W_out.grad.zero_()\n",
        "                    self.b_out.grad.zero_()\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}, Cost: {cost.item():.4f}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    # ---------------- Prediction ----------------\n",
        "    def predict(self, X):\n",
        "        with torch.no_grad():\n",
        "            probs = self.forward(X)\n",
        "            predicted = torch.argmax(probs, dim=1)\n",
        "        return predicted\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#part 2- data preperation\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import torch\n",
        "X, y = make_classification(n_samples=1000, n_classes=3,\n",
        "                           n_clusters_per_class=1,\n",
        "                           n_features=2, n_informative=2, n_redundant=0, random_state=1234, flip_y=0.15)\n",
        "# Step 2: Split into train and test (avoid leakage)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Standardize features using only training data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: One-hot encode labels (multi-class targets)\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test_onehot = encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Step 5: Convert everything to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train_onehot, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test_onehot, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "YhRHExzQN-eK"
      },
      "id": "YhRHExzQN-eK",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#part 3.1- one hidden layer- varying number of hidden units- ReLU as activation function\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "# Hidden layer sizes to test\n",
        "hidden_dims = [5, 10, 20, 50, 100]\n",
        "\n",
        "results = {}\n",
        "\n",
        "for h in hidden_dims:\n",
        "    print(f\"\\nTraining model with {h} hidden units...\")\n",
        "\n",
        "    # 1. Instantiate model with 2 hidden layers\n",
        "    multiclass_mlp_model = MultiClassMLP(\n",
        "        input_dim=X_train.shape[1],\n",
        "        hidden_dim1=h,         # first hidden layer\n",
        "        hidden_dim2=h,         # second hidden layer (can be same or different)\n",
        "        output_dim=3,          # 3 classes\n",
        "        learning_rate=0.01\n",
        "    )\n",
        "\n",
        "    # 2. Train model\n",
        "    multiclass_mlp_model.fit(X_train, y_train, n_epochs=50)\n",
        "\n",
        "    # 3. Predict on training and test sets\n",
        "    y_pred_train = multiclass_mlp_model.predict(X_train).numpy()\n",
        "    y_pred_test = multiclass_mlp_model.predict(X_test).numpy()\n",
        "\n",
        "    # Convert one-hot test labels to class indices\n",
        "    y_true_train = torch.argmax(y_train, dim=1).numpy()\n",
        "    y_true_test = torch.argmax(y_test, dim=1).numpy()\n",
        "\n",
        "    # 4. Compute accuracy\n",
        "    train_acc = accuracy_score(y_true_train, y_pred_train)\n",
        "    test_acc = accuracy_score(y_true_test, y_pred_test)\n",
        "\n",
        "    results[h] = {\"train_acc\": train_acc, \"test_acc\": test_acc}\n",
        "    print(f\"â†’ Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "# 5. Summary of results\n",
        "print(\"\\nPerformance summary:\")\n",
        "for h, metrics in results.items():\n",
        "    print(f\"Hidden units: {h:3d} | Train Acc: {metrics['train_acc']:.4f} | Test Acc: {metrics['test_acc']:.4f}\")"
      ],
      "metadata": {
        "id": "zfZxJp04PafK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "176bd4dc-5554-4b87-a41b-1e01bd751a27"
      },
      "id": "zfZxJp04PafK",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model with 5 hidden units...\n",
            "Epoch 0, Cost: 0.7541\n",
            "Epoch 10, Cost: 0.1853\n",
            "Epoch 20, Cost: 0.2302\n",
            "Epoch 30, Cost: 1.8952\n",
            "Epoch 40, Cost: 0.1648\n",
            "â†’ Train Acc: 0.8075, Test Acc: 0.7850\n",
            "\n",
            "Training model with 10 hidden units...\n",
            "Epoch 0, Cost: 0.1088\n",
            "Epoch 10, Cost: 0.2316\n",
            "Epoch 20, Cost: 0.4442\n",
            "Epoch 30, Cost: 1.9294\n",
            "Epoch 40, Cost: 0.0988\n",
            "â†’ Train Acc: 0.8013, Test Acc: 0.7950\n",
            "\n",
            "Training model with 20 hidden units...\n",
            "Epoch 0, Cost: 0.9895\n",
            "Epoch 10, Cost: 0.0816\n",
            "Epoch 20, Cost: 0.9297\n",
            "Epoch 30, Cost: 1.1835\n",
            "Epoch 40, Cost: 0.4996\n",
            "â†’ Train Acc: 0.8263, Test Acc: 0.7900\n",
            "\n",
            "Training model with 50 hidden units...\n",
            "Epoch 0, Cost: nan\n",
            "Epoch 10, Cost: nan\n",
            "Epoch 20, Cost: nan\n",
            "Epoch 30, Cost: nan\n",
            "Epoch 40, Cost: nan\n",
            "â†’ Train Acc: 0.3350, Test Acc: 0.3200\n",
            "\n",
            "Training model with 100 hidden units...\n",
            "Epoch 0, Cost: nan\n",
            "Epoch 10, Cost: nan\n",
            "Epoch 20, Cost: nan\n",
            "Epoch 30, Cost: nan\n",
            "Epoch 40, Cost: nan\n",
            "â†’ Train Acc: 0.3350, Test Acc: 0.3200\n",
            "\n",
            "Performance summary:\n",
            "Hidden units:   5 | Train Acc: 0.8075 | Test Acc: 0.7850\n",
            "Hidden units:  10 | Train Acc: 0.8013 | Test Acc: 0.7950\n",
            "Hidden units:  20 | Train Acc: 0.8263 | Test Acc: 0.7900\n",
            "Hidden units:  50 | Train Acc: 0.3350 | Test Acc: 0.3200\n",
            "Hidden units: 100 | Train Acc: 0.3350 | Test Acc: 0.3200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#part 3.2 - Same number of hidden units across one or two hidden layers - ReLU as activation function\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "# List of hidden units to test\n",
        "hidden_units = [5, 10, 20, 50]\n",
        "\n",
        "results = {}\n",
        "\n",
        "for h in hidden_units:\n",
        "    print(f\"\\n=== Testing {h} hidden units per layer ===\")\n",
        "\n",
        "    # ---------- 1 hidden layer ----------\n",
        "    model_1hl = MultiClassMLP(\n",
        "        input_dim=X_train.shape[1],\n",
        "        hidden_dim1=h,\n",
        "        hidden_dim2=h,       # for 1 hidden layer, we'll ignore the second layer in forward or set to None if your class allows\n",
        "        output_dim=3,\n",
        "        learning_rate=0.01\n",
        "    )\n",
        "\n",
        "    # You can adjust forward to skip second hidden layer if needed for 1-layer test\n",
        "    # For simplicity here, we'll just compare 2-layer models with same units\n",
        "\n",
        "    # Train model\n",
        "    model_1hl.fit(X_train, y_train, n_epochs=50)\n",
        "\n",
        "    # Predict\n",
        "    y_pred_train = model_1hl.predict(X_train).numpy()\n",
        "    y_pred_test = model_1hl.predict(X_test).numpy()\n",
        "\n",
        "    y_true_train = torch.argmax(y_train, dim=1).numpy()\n",
        "    y_true_test = torch.argmax(y_test, dim=1).numpy()\n",
        "\n",
        "    train_acc = accuracy_score(y_true_train, y_pred_train)\n",
        "    test_acc = accuracy_score(y_true_test, y_pred_test)\n",
        "\n",
        "    results[f\"{h}-1hl\"] = {\"train_acc\": train_acc, \"test_acc\": test_acc}\n",
        "    print(f\"1 Hidden Layer â†’ Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "    # ---------- 2 hidden layers ----------\n",
        "    model_2hl = MultiClassMLP(\n",
        "        input_dim=X_train.shape[1],\n",
        "        hidden_dim1=h,\n",
        "        hidden_dim2=h,  # second hidden layer\n",
        "        output_dim=3,\n",
        "        learning_rate=0.01\n",
        "    )\n",
        "\n",
        "    model_2hl.fit(X_train, y_train, n_epochs=50)\n",
        "\n",
        "    y_pred_train2 = model_2hl.predict(X_train).numpy()\n",
        "    y_pred_test2 = model_2hl.predict(X_test).numpy()\n",
        "\n",
        "    train_acc2 = accuracy_score(y_true_train, y_pred_train2)\n",
        "    test_acc2 = accuracy_score(y_true_test, y_pred_test2)\n",
        "\n",
        "    results[f\"{h}-2hl\"] = {\"train_acc\": train_acc2, \"test_acc\": test_acc2}\n",
        "    print(f\"2 Hidden Layers â†’ Train Acc: {train_acc2:.4f}, Test Acc: {test_acc2:.4f}\")\n",
        "\n",
        "# ---------- Summary ----------\n",
        "print(\"\\n=== Performance Summary ===\")\n",
        "for key, metrics in results.items():\n",
        "    print(f\"{key} | Train Acc: {metrics['train_acc']:.4f} | Test Acc: {metrics['test_acc']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BW0WsO6A8TD",
        "outputId": "71895dc6-a554-48e5-f57f-6fddad785f98"
      },
      "id": "1BW0WsO6A8TD",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Testing 5 hidden units per layer ===\n",
            "Epoch 0, Cost: 0.7541\n",
            "Epoch 10, Cost: 0.1853\n",
            "Epoch 20, Cost: 0.2302\n",
            "Epoch 30, Cost: 1.8952\n",
            "Epoch 40, Cost: 0.1648\n",
            "1 Hidden Layer â†’ Train Acc: 0.8075, Test Acc: 0.7850\n",
            "Epoch 0, Cost: 0.6461\n",
            "Epoch 10, Cost: 0.1503\n",
            "Epoch 20, Cost: 2.7269\n",
            "Epoch 30, Cost: 0.2269\n",
            "Epoch 40, Cost: 0.0610\n",
            "2 Hidden Layers â†’ Train Acc: 0.8075, Test Acc: 0.8000\n",
            "\n",
            "=== Testing 10 hidden units per layer ===\n",
            "Epoch 0, Cost: 0.0316\n",
            "Epoch 10, Cost: 0.3140\n",
            "Epoch 20, Cost: 2.1872\n",
            "Epoch 30, Cost: 1.4099\n",
            "Epoch 40, Cost: 0.0009\n",
            "1 Hidden Layer â†’ Train Acc: 0.8175, Test Acc: 0.8000\n",
            "Epoch 0, Cost: 1.8301\n",
            "Epoch 10, Cost: 0.5501\n",
            "Epoch 20, Cost: 0.1744\n",
            "Epoch 30, Cost: 0.2358\n",
            "Epoch 40, Cost: 2.8817\n",
            "2 Hidden Layers â†’ Train Acc: 0.8213, Test Acc: 0.8000\n",
            "\n",
            "=== Testing 20 hidden units per layer ===\n",
            "Epoch 0, Cost: 0.0045\n",
            "Epoch 10, Cost: 0.2070\n",
            "Epoch 20, Cost: 0.3825\n",
            "Epoch 30, Cost: 0.3369\n",
            "Epoch 40, Cost: 0.4455\n",
            "1 Hidden Layer â†’ Train Acc: 0.8237, Test Acc: 0.8000\n",
            "Epoch 0, Cost: nan\n",
            "Epoch 10, Cost: nan\n",
            "Epoch 20, Cost: nan\n",
            "Epoch 30, Cost: nan\n",
            "Epoch 40, Cost: nan\n",
            "2 Hidden Layers â†’ Train Acc: 0.3350, Test Acc: 0.3200\n",
            "\n",
            "=== Testing 50 hidden units per layer ===\n",
            "Epoch 0, Cost: nan\n",
            "Epoch 10, Cost: nan\n",
            "Epoch 20, Cost: nan\n",
            "Epoch 30, Cost: nan\n",
            "Epoch 40, Cost: nan\n",
            "1 Hidden Layer â†’ Train Acc: 0.3350, Test Acc: 0.3200\n",
            "Epoch 0, Cost: nan\n",
            "Epoch 10, Cost: nan\n",
            "Epoch 20, Cost: nan\n",
            "Epoch 30, Cost: nan\n",
            "Epoch 40, Cost: nan\n",
            "2 Hidden Layers â†’ Train Acc: 0.3350, Test Acc: 0.3200\n",
            "\n",
            "=== Performance Summary ===\n",
            "5-1hl | Train Acc: 0.8075 | Test Acc: 0.7850\n",
            "5-2hl | Train Acc: 0.8075 | Test Acc: 0.8000\n",
            "10-1hl | Train Acc: 0.8175 | Test Acc: 0.8000\n",
            "10-2hl | Train Acc: 0.8213 | Test Acc: 0.8000\n",
            "20-1hl | Train Acc: 0.8237 | Test Acc: 0.8000\n",
            "20-2hl | Train Acc: 0.3350 | Test Acc: 0.3200\n",
            "50-1hl | Train Acc: 0.3350 | Test Acc: 0.3200\n",
            "50-2hl | Train Acc: 0.3350 | Test Acc: 0.3200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#part 3.3- One hidden layer, varying the number of hidden units (use Tanh as their activation function)\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "# List of hidden layer sizes to test\n",
        "hidden_units = [5, 10, 20, 50, 100]\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Define a new class for 1-hidden-layer MLP with Tanh activation\n",
        "class MultiClassMLP_Tanh:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.01):\n",
        "        self.W_1 = torch.randn(input_dim, hidden_dim, requires_grad=True)\n",
        "        self.b_1 = torch.randn(hidden_dim, requires_grad=True)\n",
        "        self.W_out = torch.randn(hidden_dim, output_dim, requires_grad=True)\n",
        "        self.b_out = torch.randn(output_dim, requires_grad=True)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def tanh(self, z):\n",
        "        return torch.tanh(z)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        return torch.exp(x) / torch.exp(x).sum(dim=1, keepdim=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        h = torch.matmul(X, self.W_1) + self.b_1\n",
        "        h_tanh = self.tanh(h)\n",
        "        out = torch.matmul(h_tanh, self.W_out) + self.b_out\n",
        "        return self.softmax(out)\n",
        "\n",
        "    def categorical_cross_entropy(self, y_pred, y_true):\n",
        "        eps = torch.finfo(y_pred.dtype).eps\n",
        "        y_pred = torch.clamp(y_pred, eps, 1 - eps)\n",
        "        return -torch.mean(torch.sum(y_true * torch.log(y_pred), dim=1))\n",
        "\n",
        "    def fit(self, X, y, n_epochs=100):\n",
        "        for epoch in range(n_epochs):\n",
        "            randperm = torch.randperm(X.shape[0])\n",
        "            for ii in randperm:\n",
        "                x_batch = X[ii].unsqueeze(0)\n",
        "                y_batch = y[ii].unsqueeze(0)\n",
        "\n",
        "                probs = self.forward(x_batch)\n",
        "                cost = self.categorical_cross_entropy(probs, y_batch)\n",
        "                cost.backward()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    self.W_1 -= self.learning_rate * self.W_1.grad\n",
        "                    self.b_1 -= self.learning_rate * self.b_1.grad\n",
        "                    self.W_out -= self.learning_rate * self.W_out.grad\n",
        "                    self.b_out -= self.learning_rate * self.b_out.grad\n",
        "\n",
        "                    self.W_1.grad.zero_()\n",
        "                    self.b_1.grad.zero_()\n",
        "                    self.W_out.grad.zero_()\n",
        "                    self.b_out.grad.zero_()\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}, Cost: {cost.item():.4f}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        with torch.no_grad():\n",
        "            probs = self.forward(X)\n",
        "            predicted = torch.argmax(probs, dim=1)\n",
        "        return predicted\n",
        "\n",
        "# ---------- Train and evaluate ----------\n",
        "for h in hidden_units:\n",
        "    print(f\"\\nTraining 1-hidden-layer model with {h} units (Tanh)...\")\n",
        "    model = MultiClassMLP_Tanh(input_dim=X_train.shape[1], hidden_dim=h, output_dim=3, learning_rate=0.01)\n",
        "    model.fit(X_train, y_train, n_epochs=50)\n",
        "\n",
        "    y_pred_train = model.predict(X_train).numpy()\n",
        "    y_pred_test = model.predict(X_test).numpy()\n",
        "\n",
        "    y_true_train = torch.argmax(y_train, dim=1).numpy()\n",
        "    y_true_test = torch.argmax(y_test, dim=1).numpy()\n",
        "\n",
        "    train_acc = accuracy_score(y_true_train, y_pred_train)\n",
        "    test_acc = accuracy_score(y_true_test, y_pred_test)\n",
        "\n",
        "    results[h] = {\"train_acc\": train_acc, \"test_acc\": test_acc}\n",
        "    print(f\"â†’ Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "# ---------- Summary ----------\n",
        "print(\"\\nPerformance summary (Tanh):\")\n",
        "for h, metrics in results.items():\n",
        "    print(f\"Hidden units: {h:3d} | Train Acc: {metrics['train_acc']:.4f} | Test Acc: {metrics['test_acc']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCeW01MzA9cc",
        "outputId": "284b4286-ef80-4aef-8f36-a3c687ebee6d"
      },
      "id": "oCeW01MzA9cc",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training 1-hidden-layer model with 5 units (Tanh)...\n",
            "Epoch 0, Cost: 0.2956\n",
            "Epoch 10, Cost: 0.3922\n",
            "Epoch 20, Cost: 0.1392\n",
            "Epoch 30, Cost: 0.2840\n",
            "Epoch 40, Cost: 0.3862\n",
            "â†’ Train Acc: 0.8125, Test Acc: 0.7800\n",
            "\n",
            "Training 1-hidden-layer model with 10 units (Tanh)...\n",
            "Epoch 0, Cost: 0.7430\n",
            "Epoch 10, Cost: 0.1674\n",
            "Epoch 20, Cost: 0.0714\n",
            "Epoch 30, Cost: 0.1640\n",
            "Epoch 40, Cost: 0.0861\n",
            "â†’ Train Acc: 0.8050, Test Acc: 0.7850\n",
            "\n",
            "Training 1-hidden-layer model with 20 units (Tanh)...\n",
            "Epoch 0, Cost: 0.6556\n",
            "Epoch 10, Cost: 2.7477\n",
            "Epoch 20, Cost: 3.1624\n",
            "Epoch 30, Cost: 0.1481\n",
            "Epoch 40, Cost: 0.0213\n",
            "â†’ Train Acc: 0.8113, Test Acc: 0.8000\n",
            "\n",
            "Training 1-hidden-layer model with 50 units (Tanh)...\n",
            "Epoch 0, Cost: 0.0064\n",
            "Epoch 10, Cost: 0.0821\n",
            "Epoch 20, Cost: 0.5028\n",
            "Epoch 30, Cost: 0.9176\n",
            "Epoch 40, Cost: 0.1576\n",
            "â†’ Train Acc: 0.8187, Test Acc: 0.8150\n",
            "\n",
            "Training 1-hidden-layer model with 100 units (Tanh)...\n",
            "Epoch 0, Cost: 0.7957\n",
            "Epoch 10, Cost: 0.6381\n",
            "Epoch 20, Cost: 0.7280\n",
            "Epoch 30, Cost: 0.1860\n",
            "Epoch 40, Cost: 0.0039\n",
            "â†’ Train Acc: 0.8100, Test Acc: 0.7950\n",
            "\n",
            "Performance summary (Tanh):\n",
            "Hidden units:   5 | Train Acc: 0.8125 | Test Acc: 0.7800\n",
            "Hidden units:  10 | Train Acc: 0.8050 | Test Acc: 0.7850\n",
            "Hidden units:  20 | Train Acc: 0.8113 | Test Acc: 0.8000\n",
            "Hidden units:  50 | Train Acc: 0.8187 | Test Acc: 0.8150\n",
            "Hidden units: 100 | Train Acc: 0.8100 | Test Acc: 0.7950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Same number of hidden units across one or two hidden layers (use Tanh as their activation function)\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Hidden units to test\n",
        "hidden_units = [5, 10, 20, 50]\n",
        "\n",
        "results = {}\n",
        "\n",
        "for h in hidden_units:\n",
        "    print(f\"\\n=== Testing {h} hidden units per layer ===\")\n",
        "\n",
        "    # ---------- 1 hidden layer ----------\n",
        "    model_1hl = MultiClassMLP_2Hidden_Tanh(\n",
        "        input_dim=X_train.shape[1],\n",
        "        hidden_dim1=h,\n",
        "        hidden_dim2=None,  # None means skip second hidden layer\n",
        "        output_dim=3,\n",
        "        learning_rate=0.01\n",
        "    )\n",
        "    print(f\"\\nTraining 1-hidden-layer model with {h} units (Tanh)...\")\n",
        "    model_1hl.fit(X_train, y_train, n_epochs=50)\n",
        "\n",
        "    y_pred_train1 = model_1hl.predict(X_train).numpy()\n",
        "    y_pred_test1 = model_1hl.predict(X_test).numpy()\n",
        "\n",
        "    y_true_train = torch.argmax(y_train, dim=1).numpy()\n",
        "    y_true_test = torch.argmax(y_test, dim=1).numpy()\n",
        "\n",
        "    train_acc1 = accuracy_score(y_true_train, y_pred_train1)\n",
        "    test_acc1 = accuracy_score(y_true_test, y_pred_test1)\n",
        "    print(f\"1 Hidden Layer â†’ Train Acc: {train_acc1:.4f}, Test Acc: {test_acc1:.4f}\")\n",
        "\n",
        "    # ---------- 2 hidden layers ----------\n",
        "    model_2hl = MultiClassMLP_2Hidden_Tanh(\n",
        "        input_dim=X_train.shape[1],\n",
        "        hidden_dim1=h,\n",
        "        hidden_dim2=h,  # second hidden layer\n",
        "        output_dim=3,\n",
        "        learning_rate=0.01\n",
        "    )\n",
        "    print(f\"\\nTraining 2-hidden-layer model with {h} units per layer (Tanh)...\")\n",
        "    model_2hl.fit(X_train, y_train, n_epochs=50)\n",
        "\n",
        "    y_pred_train2 = model_2hl.predict(X_train).numpy()\n",
        "    y_pred_test2 = model_2hl.predict(X_test).numpy()\n",
        "\n",
        "    train_acc2 = accuracy_score(y_true_train, y_pred_train2)\n",
        "    test_acc2 = accuracy_score(y_true_test, y_pred_test2)\n",
        "    print(f\"2 Hidden Layers â†’ Train Acc: {train_acc2:.4f}, Test Acc: {test_acc2:.4f}\")\n",
        "\n",
        "    results[h] = {\n",
        "        \"1hl\": {\"train_acc\": train_acc1, \"test_acc\": test_acc1},\n",
        "        \"2hl\": {\"train_acc\": train_acc2, \"test_acc\": test_acc2}\n",
        "    }\n",
        "\n",
        "# ----------Summary----------\n",
        "print(\"\\n=== Performance summary (Tanh) ===\")\n",
        "for h, metrics in results.items():\n",
        "    print(f\"{h} units per layer | \"\n",
        "          f\"1HL â†’ Train: {metrics['1hl']['train_acc']:.4f}, Test: {metrics['1hl']['test_acc']:.4f} | \"\n",
        "          f\"2HL â†’ Train: {metrics['2hl']['train_acc']:.4f}, Test: {metrics['2hl']['test_acc']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0sUiI7QA-cV",
        "outputId": "45b3c13e-92f8-412f-a921-29f849a35d68"
      },
      "id": "F0sUiI7QA-cV",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Testing 5 hidden units per layer ===\n",
            "\n",
            "Training 1-hidden-layer model with 5 units (Tanh)...\n",
            "Epoch 0, Cost: 0.2005\n",
            "Epoch 10, Cost: 0.0542\n",
            "Epoch 20, Cost: 0.1312\n",
            "Epoch 30, Cost: 0.0936\n",
            "Epoch 40, Cost: 1.7767\n",
            "1 Hidden Layer â†’ Train Acc: 0.8137, Test Acc: 0.7950\n",
            "\n",
            "Training 2-hidden-layer model with 5 units per layer (Tanh)...\n",
            "Epoch 0, Cost: 0.1849\n",
            "Epoch 10, Cost: 0.2050\n",
            "Epoch 20, Cost: 0.2379\n",
            "Epoch 30, Cost: 0.1435\n",
            "Epoch 40, Cost: 0.1159\n",
            "2 Hidden Layers â†’ Train Acc: 0.8100, Test Acc: 0.7900\n",
            "\n",
            "=== Testing 10 hidden units per layer ===\n",
            "\n",
            "Training 1-hidden-layer model with 10 units (Tanh)...\n",
            "Epoch 0, Cost: 0.1699\n",
            "Epoch 10, Cost: 0.5133\n",
            "Epoch 20, Cost: 0.2876\n",
            "Epoch 30, Cost: 0.1317\n",
            "Epoch 40, Cost: 0.1635\n",
            "1 Hidden Layer â†’ Train Acc: 0.8050, Test Acc: 0.7800\n",
            "\n",
            "Training 2-hidden-layer model with 10 units per layer (Tanh)...\n",
            "Epoch 0, Cost: 0.1479\n",
            "Epoch 10, Cost: 0.1388\n",
            "Epoch 20, Cost: 0.1118\n",
            "Epoch 30, Cost: 1.2206\n",
            "Epoch 40, Cost: 0.0809\n",
            "2 Hidden Layers â†’ Train Acc: 0.8200, Test Acc: 0.8000\n",
            "\n",
            "=== Testing 20 hidden units per layer ===\n",
            "\n",
            "Training 1-hidden-layer model with 20 units (Tanh)...\n",
            "Epoch 0, Cost: 0.4421\n",
            "Epoch 10, Cost: 0.0733\n",
            "Epoch 20, Cost: 0.0208\n",
            "Epoch 30, Cost: 0.1962\n",
            "Epoch 40, Cost: 0.0872\n",
            "1 Hidden Layer â†’ Train Acc: 0.8200, Test Acc: 0.7950\n",
            "\n",
            "Training 2-hidden-layer model with 20 units per layer (Tanh)...\n",
            "Epoch 0, Cost: 0.0650\n",
            "Epoch 10, Cost: 0.4696\n",
            "Epoch 20, Cost: 3.3712\n",
            "Epoch 30, Cost: 0.0514\n",
            "Epoch 40, Cost: 0.4711\n",
            "2 Hidden Layers â†’ Train Acc: 0.8313, Test Acc: 0.8050\n",
            "\n",
            "=== Testing 50 hidden units per layer ===\n",
            "\n",
            "Training 1-hidden-layer model with 50 units (Tanh)...\n",
            "Epoch 0, Cost: 0.5161\n",
            "Epoch 10, Cost: 0.0329\n",
            "Epoch 20, Cost: 3.6114\n",
            "Epoch 30, Cost: 0.3165\n",
            "Epoch 40, Cost: 0.7584\n",
            "1 Hidden Layer â†’ Train Acc: 0.8087, Test Acc: 0.7950\n",
            "\n",
            "Training 2-hidden-layer model with 50 units per layer (Tanh)...\n",
            "Epoch 0, Cost: 0.8432\n",
            "Epoch 10, Cost: 1.8013\n",
            "Epoch 20, Cost: 0.4617\n",
            "Epoch 30, Cost: 3.0947\n",
            "Epoch 40, Cost: 0.6216\n",
            "2 Hidden Layers â†’ Train Acc: 0.8075, Test Acc: 0.7750\n",
            "\n",
            "=== Performance summary (Tanh) ===\n",
            "5 units per layer | 1HL â†’ Train: 0.8137, Test: 0.7950 | 2HL â†’ Train: 0.8100, Test: 0.7900\n",
            "10 units per layer | 1HL â†’ Train: 0.8050, Test: 0.7800 | 2HL â†’ Train: 0.8200, Test: 0.8000\n",
            "20 units per layer | 1HL â†’ Train: 0.8200, Test: 0.7950 | 2HL â†’ Train: 0.8313, Test: 0.8050\n",
            "50 units per layer | 1HL â†’ Train: 0.8087, Test: 0.7950 | 2HL â†’ Train: 0.8075, Test: 0.7750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. ***REFLECTION AND DISCUSSION***\n",
        "\n",
        "Reflect on the impact of the different hyperparameter settings:\n",
        "\n",
        "- How does the number of hidden units affect performance?\n",
        "When using the ReLU activation function, model performance initially improves as the number of hidden units increases from 5 to 20, reflected by slightly higher training and testing accuracies. This suggests that adding more units initially helps the network learn richer representations of the data. However, when the number of hidden units increases to 50 and 100, both training and test accuracies drop sharply. This likely indicates overfitting or unstable training, as larger networks can become harder to optimize and may require smaller learning rates or more regularization.\n",
        "In contrast, with the Tanh activation function, performance improves more steadily as the number of hidden units increases from 5 to 50, showing that moderate model complexity helps capture more non-linear patterns. However, at 100 hidden units, both training and testing accuracy slightly decline, suggesting diminishing returns and possible overfitting.\n",
        "Overall, both activation functions show that increasing hidden units improves performance up to a point, after which accuracy either plateaus or decreases. Moderate network sizes (around 20â€“50 hidden units) seem to provide the best balance between model flexibility and generalization.\n",
        "\n",
        "- What changes when using two layers instead of one?\n",
        "When using the Tanh activation function, performance improves as the number of hidden units increases from 5 to 20, for both one and two hidden layers. Both training and test accuracies rise slightly, showing that increasing model capacity helps capture more patterns in the data. However, at 50 units per layer, accuracy begins to decline marginally, suggesting that additional complexity does not necessarily translate to better generalization. The difference between one and two hidden layers is relatively small, though the two-layer models tend to perform just slightly better overall.\n",
        "In contrast, with the ReLU activation function, accuracy improves up to 20 hidden units but drops sharply at 50 and 100 units. This pattern suggests that the larger ReLU networks may be overfitting or experiencing vanishing gradient issues, leading to poor learning stability.\n",
        "Overall, moderate network sizes (around 20 hidden units) consistently yield the best results for both activations. Tanh shows more stable and consistent performance across, while ReLU is more sensitive to the number of hidden units, especially as the network grows deeper or wider.\n",
        "\n",
        "- How does the activation function (ReLU vs. Tanh) influence results?\n",
        "The activation function strongly influences how effectively the model learns and generalizes. In this experiment, models using Tanh achieved more stable and consistent performance as hidden units increased, with both training and test accuracy improving gradually before leveling off. In contrast, ReLU models performed well with fewer hidden units but showed a sharp decline in accuracy for larger number of hidden units, likely due to issues such as dead neurons and unstable gradients. Overall, Tanh led to smoother learning and better generalization across network sizes, while ReLU was faster initially but less reliable as model complexity increased.\n",
        "\n"
      ],
      "metadata": {
        "id": "_OIIpt0GM2rQ"
      },
      "id": "_OIIpt0GM2rQ"
    },
    {
      "cell_type": "markdown",
      "id": "82d530fb-4c1b-40cb-a107-d897e8764550",
      "metadata": {
        "id": "82d530fb-4c1b-40cb-a107-d897e8764550"
      },
      "source": [
        "## 3: Regression Task (25 points)\n",
        "\n",
        "Use the dataset below to complete points 1 to 4 in the general instructions for this task.\n",
        "\n",
        "Use as many cells as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bed90c4d-bf3c-40b9-bd53-0c6d42f88aed",
      "metadata": {
        "id": "bed90c4d-bf3c-40b9-bd53-0c6d42f88aed"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "X, y = make_regression(n_samples=1000, n_features=2, n_informative=2, random_state=1234, noise=75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58b46422-31ee-4534-bc09-9bda8cee5816",
      "metadata": {
        "id": "58b46422-31ee-4534-bc09-9bda8cee5816"
      },
      "outputs": [],
      "source": [
        "# USE AS MANY CELLS AS NEEDED"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8824282-54a8-42da-be16-105be2c403fe",
      "metadata": {
        "id": "f8824282-54a8-42da-be16-105be2c403fe"
      },
      "source": [
        "## 4. Discussion (5 points)\n",
        "\n",
        "You created a separate class for each task and likely repeated much of the same code across implementations.\n",
        "\n",
        "Discuss within your group how could you have leveraged inheritance to make your code more reusable and avoid duplication. Provide examples. Be specific."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67e74115-ce71-45e8-8fae-4ec8c2a45685",
      "metadata": {
        "id": "67e74115-ce71-45e8-8fae-4ec8c2a45685"
      },
      "source": [
        "YOUR TEXT HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcc5a8a9-e125-411f-9c7e-066eee0999f5",
      "metadata": {
        "id": "bcc5a8a9-e125-411f-9c7e-066eee0999f5"
      },
      "source": [
        "## 5. Collaboration Reflection (5 points)\n",
        "\n",
        "As a group, briefly reflect on the following (max 1â€“2 short paragraphs):\n",
        "\n",
        "- How did the group dynamics work throughout the assignment?\n",
        "- Were there any major disagreements or diverging approaches?\n",
        "- How did you resolve conflicts or make final modeling decisions?\n",
        "- What did you learn from each other during this project?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af0ade16-f0b9-440c-adbe-a135fc88e807",
      "metadata": {
        "id": "af0ade16-f0b9-440c-adbe-a135fc88e807"
      },
      "source": [
        "YOUR TEXT HERE"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}